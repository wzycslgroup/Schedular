# Schedular

**勿忘初心**

**起风了，唯有努力生存**

# 目录

- [1 目标](#目标)
- [2 学习计划](#学习计划)
- [3 参考课程](#参考课程)
- [4 参考书目](#参考书目)

## 1 目标

掌握机器学习的基本算法，能够熟练推导算法的相关公式。

## 2 学习计划

### 2.1 学习内容（根据学习情况灵活取舍）

**我们不一定也不可能学完所有的topic,根据实际情况进行取舍即可.**


<table id="topics" class="table table-bordered no-more-tables">
  <thead class="active" style="background-color:#f9f9f9">
    <th>Category</th><th>Topic</th>
  </thead>

  <tbody>
  <!--<tr>
    <td colspan="4" style="text-align:center; vertical-align:middle;background-color:#fffde7">
      <strong>Introduction</strong> (1 class)
    </td>
  </tr>-->
  <tr>
    <td>Review</td> <td> 
      <ul>
	<li> Linear Algebra
	<li> Matrix Calculus
        <li> Probability and Statistics
	</ul>
      </td>
   </tr>

  <tr>
    <td>Supervised Learning</td> 
    <td> <ul>
	<li> Linear Regression (Gradient Descent, Normal Equations)
	<li> Weighted Linear Regression (LWR)
	<li> Logistic Regression, Perceptron
         <li> Newton's Method, KL-divergence, (cross-)Entropy, Natural Gradient
	 <li> Exponential Family and Generalized Linear Models
         <li> Generative Models (Gaussian Discriminant Analysis, Naive Bayes)
         <li> Kernel Method (SVM, Gaussian Processes)
	 <li> Tree Ensembles (Decision trees, Random Forests, Boosting and Gradient Boosting)
	</ul> 
    </td>
  </tr>

  <tr>
     <td> Learning Theory </td>
     <td> <ul>
            <li> Regularization
             <li> Bias-Variance Decomposition and Tradeoff
            <li> Concentration Inequalities
            <li> Generalization and Uniform Convergence
            <li> VC-dimension
           </ul>
      </td>
   </tr>

   <tr>
     <td> Deep Learning </td>
     <td> <ul> <li> Neural Networks <li> Backpropagation <li> Deep Architectures </ul> </td>
   </tr>

   <tr>
     <td> Unsupervised Learning </td>
     <td> <ul>
	 <li> K-means
	 <li> Gaussian Mixture Model (GMM)
	 <li> Expectation Maximization (EM)
	 <li> Variational Auto-encoder (VAE)
	 <li> Factor Analysis
	 <li> Principal Components Analysis (PCA)
	 <li> Independent Components Analysis (ICA)
     </ul> </td>
   </tr>

   <tr>
     <td> Reinforcement Learning (RL) </td>
     <td>
       <ul>
	 <li> Markov Decision Processes (MDP)
	 <li> Bellmans Equations
	 <li> Value Iteration and Policy Iteration
	 <li> Value Function Approximation
	 <li> Q-Learning
       </ul>
     </td>
   </tr>

   <tr>
     <td> Application </td>
     <td>
       <ul>
	 <li> Advice on structuring an ML project
	 <li> Evaluation Metrics
       </ul>
     </td>
</table>

### 2.2 时间安排

暂定于每周日上午讲解

| 时间            | 讲解内容                                |
| -------------- | ----------------------------------- |
| 2019年7月15日~2019年7月21日 | NaiveBayes(C)<br>Perceptron(W&C)<br>SVM(W)|
| 2019年7月22日~2019年7月28日 | Kernel Method(W)<br>Linear Reagression(C)<br>Model Selection(C) |
| 2019年7月29日~2019年8月4日  | PCA(C)<br>SVD(W)<br>VC Dimension(W&C) |
| 2019年8月5日~2019年8月11日 | Bagging(C)<br>Boosting(W) |
| 2019年8月12日~2019年8月18日 | EM(C&W) |

### 2.3 注意事项

- 讲解人需要在<font size=4>**每周四晚上之前**</font>上传本周所讲内容的相关资料

- 讲解人在讲解之前需要<font size=4>**事先**</font>准备好讲稿，并根据自己的理解命制相应的测试题

- 每个人都要在讲解之前对本周所讲的内容有一定的认识。

- **<font size=4>时间是挤出来的，我们一定不要忘了暑假的目标！理论上来说，每天抽出一到两个小时学习相关内容，到了周日上午，是足以完成学习计划的。</font>** 

## 3 参考课程

- [Stanford CS229](http://cs229.stanford.edu/syllabus.html)

- [UW cousera Machine Learning](https://www.coursera.org/courses?query=machine%20learning%20washington)

- [CMU CS10-601](http://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html)

- [Cornell CS4780](http://www.cs.cornell.edu/courses/cs4780/2018fa/page18/index.html)

- [机器学习基石](https://www.coursera.org/instructor/htlin)

- [机器学习技法](https://www.bilibili.com/video/av36760800?from=search&seid=3780791729282001501)


## 4 参考书目

- 《统计学习方法（第二版）》
- PRML
- MLAPP
- 西瓜书











